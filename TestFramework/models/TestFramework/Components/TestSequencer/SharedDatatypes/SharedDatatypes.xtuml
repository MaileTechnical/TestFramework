-- BP 7.1.6 content: Package syschar: 3 persistence-version: 7.1.6

INSERT INTO EP_PKG
	VALUES ("6c66f96e-cbfb-43a4-a1b6-7d3c4c1a96dc",
	"00000000-0000-0000-0000-000000000000",
	"b06a8228-7533-46d6-b644-27cf49555288",
	'SharedDatatypes',
	'',
	0);
INSERT INTO GD_MD
	VALUES ("46f11303-61a8-4d7c-b17d-37dd34906945",
	112,
	"6c66f96e-cbfb-43a4-a1b6-7d3c4c1a96dc",
	108,
	0,
	0,
	1,
	1,
	1,
	12,
	1,
	0,
	0,
	0,
	0,
	0,
	'',
	'TestFramework::Components::TestSequencer::SharedDatatypes');
INSERT INTO DIM_DIA
	VALUES ("46f11303-61a8-4d7c-b17d-37dd34906945",
	'',
	1.000000,
	0.000000,
	0.000000,
	"00000000-0000-0000-0000-000000000000");
INSERT INTO EP_PKGREF
	VALUES ("6c66f96e-cbfb-43a4-a1b6-7d3c4c1a96dc",
	"bf1a6bcd-c9ee-4a11-82c1-dbb294c02033");
INSERT INTO EP_PKG_PROXY
	VALUES ("bf1a6bcd-c9ee-4a11-82c1-dbb294c02033",
	"b06a8228-7533-46d6-b644-27cf49555288",
	"b06a8228-7533-46d6-b644-27cf49555288",
	'SharedDatatypes',
	'Datatypes which are shared with client testbenches.',
	10,
	'../../../SharedDatatypes/SharedDatatypes.xtuml');
INSERT INTO PE_PE
	VALUES ("6c66f96e-cbfb-43a4-a1b6-7d3c4c1a96dc",
	1,
	"00000000-0000-0000-0000-000000000000",
	"6458b090-4d5d-4bec-a376-1c820254581c",
	7);
INSERT INTO C_C_PROXY
	VALUES ("6458b090-4d5d-4bec-a376-1c820254581c",
	"00000000-0000-0000-0000-000000000000",
	"00000000-0000-0000-0000-000000000000",
	'TestSequencer',
	'Overview:

This is a service to automate the running of a suite of multiple self-checking test cases 
including rudimentary reporting of results and requirements coverage.

Typically a self-checking test case has three primary components:
1. Establish pre-conditions.
2. Inject stimulus.
3. Verify (observe) post-stimulus conditions.
Often, the injection of stimuli and the observation of post-conditions
are intermixed, as the test case injects a stimulus, verifies the response(s), 
injects additional stimuli, and so on.

While stimulus actions and observation measurements are subject-matter specific, the organization 
of sequencing stimuli and adjudication of success or failure are not. Hence this service.


Summary description:

This subject matter-independent Test Sequencer service asumes the burdens of sequencing, timer 
management, requirements coverage tracking and summary reporting for multi-threaded test cases 
ordered in test buckets within a test suite.

A testbench, with subject matter-specific details of stimuli and observations, can use this 
service to manage its interaction with a system under test.

Each test case is defined in data as instances of stimuli and observations.  Test cases 
are typically organized into buckets.  For example, a testbench might have a bucket containing
all nominal (good-path) test cases and perhaps several buckets containing off-nominal (unusual
situations, error-recovery, etc.) test cases.  Or, test buckets could be organized around 
sections of the requirements specification.  The scheme for organizing test cases into buckets
is entirely at the discretion of the test engineer.

A test suite is configured to contain one or more test buckets which are added to the suite in order.
Each test bucket configures at least one test case; typically several related tests.

When all test buckets have been added to the test suite, the test suite is signalled to execute.
In turn, each bucket is signalled to configure its test cases and run them.

A test case consists of one or more sequences of stimuli and following observations.
The instances in the testbench have counterparts in the test service where sequencing is managed.
The testbench creates stimuli and observation instances which add themselves to the test service.
When the test bucket configuration is complete its test cases are run in order.
Running a test case involves appropriate invocation of the stimulus interactions with the system 
under test and evaluating the system''s response as measured by observations.


Some details:

Multiple sequences of stimuli may run concurrently in a test case execution.
A stimulus effects some changes in a system under test.
Following observations may evaluate the response of the system to such a stimulus.
Observations may evaluate sequentially in a defined order, or concurrently if so configured.
Observations may be triggered to evaluate on some occurence, or may poll, repeatedly evaluating.
Observations may be limited in duration; failure to succeed within this time constitutes failure.
Time delays can be interspersed between stimuli and observations and between observations.
Time resolution values are propagated as defaults down the hierarchy from test suite to observation and 
can be overridden at any level - from where they now become defaults.
Polling intervals can be specified indpendently of time resolution values.
All observations following a stimulus must complete before the next stimulus is injected
If all observations are found acceptable, the test case is deemed to be successful.
If any observation fails, the test case ceases execution: all sequences discontinue, and the test
case is deemed as having failed.
A test case may be associated with instances of requirements to support coverage reporting.


Establishing pre-conditions:

The service sends a message to the testbench indicating that the next test case to execute 
should be initialized.  Upon receiving this message, the testbench may establish pre-conditions
for the specified test case.  This might include configuring hardware resources (either
simulated or real), sending interface messages to the system under test, or (if the testbench
is inside the same component as the system under test) direct manipulation of the system under
test.


Test case failure:

A failing test typically indicates one of two possibilities.  Either the system under test is 
not behaving as specified or the test (or its supporting tooling) is in error.  Any test case
with one or more failing observations is deemed as having failed.  An identifier for each failing 
observation is emitted to the log, and the data (instances of stimuli and observations) associated
with the failing test case are retained, enabling many failures to be isolated through analysis of 
the log or inspection of the instance population. 


Usage:

For an example usage of the service, see the test bench for the Carpark application located here:
https://github.com/johnrwolfe/CarPark/tree/master/CarParkTestbench. 

The testbench test bucket, stimulus and observation classes must provide instances which will 
respond to the sequencing services signals to inject stimuli and initiate observations as the service 
works through the series of test cases. These signals carry identifiers of type unique_id which 
identify instances in the testbench. A practical way to satisfy these conditions is to create each 
of these testbench classes as a subtype of a test bucket, stimulus or observation class supertype. 
Pass the identifier of the supertype when adding instances to the sequencing service. 
On receipt of a signal from the service navigate to the subtype and carry out the appropriate action.
The service must be signaled with appropriate completion/failure of these actions in order to progress.

As the sequencer runs through the set of buckets that constitute a test suite it will signal an
initialization action for each test bucket. This should invoke an operation that creates the subtype 
instances of stimulus and observation for each test case, submitting identifiers in order.
When test case creation for a test bucket is complete the testbench signals for start of execution.

Stimuli are added to a stimulus sequence of which there must be at least one created for a test case. 
If there are multiple sequences of stimuli they will run concurrently.

Observations are each appended to a specific stimulus and may be separated by chosen time delays.
Where the order in which observations should evaluate after a stimulus is unpredictable, they may be 
configured to evaluate concurrently; each one must succeed to consitute completion. If, however, 
during test case execution it is determined that a triggered operation is no longer required, there 
is a provision to "remove" it without causing failure of the test case.

An observation may be configured with optional timers: one limits the duration until lack of success 
will be deemed a timeout failure; the other may determine a polling frequency for re-evaluation.
These timers may be independently configured with resolutions which may differ. The overall duration 
and the polling interval are specified as values using those configured resolution units.
Example: observations with timeouts specified in seconds may poll at some number of milliseconds.

An observation may be configured to poll, repeatedly evaluating system conditions until it completes 
by reporting success, or fails after some time limit. Alternatively, invocation of an evaluation can
be triggered by some specific occurrence in the testbench - such as receipt of a status change in 
the system under test. This is achieved by associating a specific key with the observation; invoking 
a sequence "assess" signal, with a key value, will trigger evaluation of any current observations 
with a matching key. If desired, such an ''asynchronous'' observation may be associated with a separate 
stimulus sequence, allowing other sets of observations to proceed while it waits to be triggered.
A ''hybrid'' observation may wait for an initial trigger and subsequently poll in search of success.

Associated with each stimulus sequence there is one interval timer. A stimulus or observation can 
start (or restart) the timer on a sequence. A subsequent observation may read the elapsed time since 
the timer was started - and may optionally reset the timer value on reading. Interval timer 
resolution defaults to that of the sequence time resolution, but can be overridden.',
	0,
	"00000000-0000-0000-0000-000000000000",
	0,
	'',
	'',
	'../TestSequencer.xtuml');
INSERT INTO S_SYS_PROXY
	VALUES ("b06a8228-7533-46d6-b644-27cf49555288",
	'TestFramework',
	1,
	'../../../TestFramework.xtuml');
